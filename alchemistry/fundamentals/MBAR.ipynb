{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0325809e-4725-46c9-b55e-d72fa467d43b",
   "metadata": {},
   "source": [
    "# Multistate Bennett Acceptance Ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb0c113-cee0-4a31-838d-12ef240546f4",
   "metadata": {},
   "source": [
    "The Multistate Bennett Acceptance Ratio (MBAR){cite}`Shirts2008` is a direct extension to [](BAR) as it allows for assessing data from all states, and predicting the free energy at an unsampled state. MBAR reduces to BAR in the limit that only two states are sampled. This equation of free energy calculations can also be seen as a zero-width bin [](WHAM). \n",
    "\n",
    "Much like WHAM, the free energies provided by this method are only a statistical estimator, however, MBAR has been shown to have the lowest variance estimator to date."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bd0716-2d6a-4e59-b961-f7ac99e15b37",
   "metadata": {},
   "source": [
    "## Derivation\n",
    "MBAR is derived from a set of $K \\times K$ weighting functions, $\\alpha_{i,j}(\\vec{q})$, that minimized the variance during the reweighting across the board. Starting from our [core free energy equation](core_eq), we have\n",
    "\n",
    "$\\displaystyle\\Delta A_{ij} = -\\beta^{-1} \\ln\\frac{Q_j}{Q_i}$\n",
    "\n",
    "We can also manipulate the same identity Bennett used when starting his derivation{cite}`Bennett1976` and then a one extra bit to come up with the relation\n",
    "\n",
    "$Q_i\\left\\langle\\alpha_{ij}\\exp\\left(-\\beta U_j\\right)\\right\\rangle_i = Q_j\\left\\langle\\alpha_{ij}\\exp\\left(-\\beta U_i\\right)\\right\\rangle_j$\n",
    "\n",
    "Although this may not seem like much, it does allow us to write out the following:\n",
    "\n",
    "$\\displaystyle \\sum\\limits_{j=1}^K \\frac{\\hat{Q_i}}{N_i} \\sum\\limits_{n=1}^{N_i} \\alpha_{ij}\\exp\\left(-\\beta U_j(\\vec{q}_{i,n})\\right) = \\sum\\limits_{j=1}^K \\frac{\\hat{Q_j}}{N_j} \\sum\\limits_{n=1}^{N_j} \\alpha_{ij}\\exp\\left(-\\beta U_i(\\vec{q}_{j,n})\\right)$\n",
    "\n",
    "assuming we use the empirical estimator for the expectation values of $\\left\\langle g \\right\\rangle_i = N_i^{-1}\\sum_{n=1}^{N_i}g(\\vec{q}_{i,n})$\n",
    "\n",
    "Choosing the optimal $\\alpha_{ij}$ can be done by looking through the literature at extended bridge sampling.{cite}`Tan2004` We then get an $\\alpha_{ij}$ of:\n",
    "\n",
    "$\\displaystyle \\alpha_{ij} = \\frac{N_j \\hat{c_j}^{-1}}{\\sum\\limits_{k=1}^K N_k \\hat{c_k}^{-1} \\exp\\left(-\\beta U_k \\right)}$.\n",
    "\n",
    "After making all the necessary substitutions, we can get an expression for an estimated free energy of:\n",
    "\n",
    "$\\displaystyle \\hat{A_i} = -\\beta^{-1} \\ln \\sum\\limits_{j=1}^K \\sum\\limits_{n=1}^{N_j} \\frac{\\exp\\left[-\\beta U_i\\right]}{\\sum_{k=1}^K N_k \\exp\\left[\\beta\\hat{A_k} - \\beta U_k\\right]}$.\n",
    "\n",
    "One of the first things you should notice is that we have a *single* free energy, not a difference. This is not a typo, but the free energies for a given set of states is only uniquely determined up to an additive constant. Because of this, one free energy must be taken in reference and thus we are once again calculating free energy differences.\n",
    "\n",
    "### Reduced potential \n",
    "\n",
    "It is important to note that the $U$ that appears in the MBAR derivation and equation is not only valid for potential energies, but any generalized/reduced potential as a function of pressure, volume, chemical potential, and number of particles. \n",
    "For example, in a general form, we can take some subset of the additive terms in the following to define the reduced potential $u_i(x)$ for thermodynamic state $i$:\n",
    "$\\displaystyle u_i(x) \\equiv \\beta_i [ U_i(x) + p_i V(x) + {\\bf \\mu}_i^\\mathrm{T} {\\bf N}(x) ]$\n",
    "The reduced potential function is uniquely defined by some combination of thermodynamic parameters $\\beta_i$ denoting the inverse temperature, $U_i(x)$ denoting the potential energy function, $p_i$ denoting the pressure, and ${\\bf \\mu}_i$ denoting the chemical potential of one or more components of the system.\n",
    "These latter two thermodynamic variables are conjugate to the box volume $V(x)$ and particle numbers ${\\bf N}(x)$.\n",
    "Use of the reduced potential simplifies the MBAR equations and generalizes them to the computation of arbitrary reduced free energy differences $\\Delta f_{ij} \\equiv \\beta_j A_j - \\beta_i A_i$ among states.\n",
    "$\\displaystyle \\hat{f_i} = -\\ln \\sum\\limits_{j=1}^K \\sum\\limits_{n=1}^{N_j} \\frac{\\exp\\left[-u_i(x_{nj})\\right]}{\\sum_{k=1}^K N_k \\exp\\left[\\hat{f_k} - u_k(x_{nj})\\right]}$.\n",
    "\n",
    "In the sum above $x_{nj}$ indicates the $n$th sample from the $j$th state.   Interestingly, in this formula, it actually doesn't matter which sample comes from which state, so it can be rewritten as a sum over all $N = \\sum_{j=1}^K N_j$, so that we have:\n",
    "$\\displaystyle \\hat{f_i} = -\\ln \\sum\\limits_{n=1}^{N} \\frac{\\exp\\left[-u_i(x_{n})\\right]}{\\sum_{k=1}^K N_k \\exp\\left[\\hat{f_k} - u_k(x_{n})\\right]}$\n",
    "\n",
    "Finally, one can rewrite this as:\n",
    "$\\displaystyle \\hat{f_i} = -\\ln \\left\\langle \\frac{\\exp\\left[-u_i(x_{n})\\right]}{\\sum_{k=1}^K \\frac{N_k}{N} \\exp\\left[\\hat{f_k} - u_k(x_{n})\\right]}\\right\\rangle $\n",
    "\n",
    "Where the ensemble averaged over is the **mixture ensemble** that consist of samples taken from each state $i$ $\\frac{N_i}{N}$th of the time.\n",
    "\n",
    "Remember always **the free energy will change depending on which reduced potential you use,** so please take this into account when working with MBAR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea991ee1",
   "metadata": {},
   "source": [
    "### Derivation from importance sampling from the mixture distribution.\n",
    "\n",
    "This can also be derived from the technique of importance sampling. \n",
    "\n",
    "First, the concept of Monte Carlo integration states that if we want to estimate some multidimensional expectation value $\\langle O \\rangle_i = \\int O(\\vec{q})p(\\vec{q}) d\\vec{q}$, where $p(\\vec{q})$ is a normalized distribution, it can be estimated from $N$ samples from the probability distribution $p(\\vec{q})$ as  $\\sum_{i=1}^{N} O(q_i)$.  Note that this works if we do not know the normalizing constant; we only need to draw samples proportional to $p\\vec{q}$. \n",
    "\n",
    "Importance sampling states that if we have two distributions $p_1(\\vec{q})$ and $p_2(\\vec{q})$, then we can write the integral as:\n",
    "\n",
    "$ \\displaystyle \\int O(\\vec{q}) p_1(\\vec{q}) d\\vec{q} = \\int O(\\vec{q}) p_2(\\vec{q}) \\left(\\frac{p_1(\\vec{q})}{p_2(\\vec{q})}\\right) d\\vec{q} $  \n",
    "Which can be approximated by the sum $\\sum_{i=1}^{N} O(\\vec{q}_i) \\left(\\frac{p_1(\\vec{q})}{p_2(\\vec{q})}\\right)$, which is a estimate of the expectation value $\\langle O \\rangle_i$, but estimated with samples from $p_2$.  \n",
    "\n",
    "Now define the *mixture* distribution of all $k$ states of interest, which we write as $p_m(\\vec{q}) = \\sum_i=1^K \\frac{N_k}{N_k} e^{f_k-u_k}$, where $N_k$ are the number of samples from each state and $N = \\sum_k N_k$ is the total number of samples from all states.\n",
    "\n",
    "We write the importance sampling estimator of $\\rangle O \\angle_1$ estimated using the mixture distribution, which is: $\\langle O \\rangle_1 = \\langle O \\frac{p_1}{p_m}\\rangle_{m}$.  \n",
    "\n",
    "For statistical mechanical problems, each $p_k(\\vec{q}) = e^{f_k - u_k(\\vec{q})}$, so $p_m = \\sum_{i=1}^K \\frac{N_k}{N} e^{f_k - u_k(\\vec{q})}$.  So the importance sampling estimator is \n",
    "\n",
    "$\\langle O \\rangle_1 = \\left\\langle O \\frac{e^{f_1 - u_1}}{\\sum_{i=1}^K \\frac{N_k}{N} e^{f_k - u_k}}\\right\\rangle_{m}$.  \n",
    "\n",
    "If we now set $O(\\vec{q})=1$, then:\n",
    "\n",
    "$ \\displaystyle 1 = \\left\\langle \\frac{e^{f_1 - u_1(\\vec{q})}}{\\sum_{i=1}^K \\frac{N_k}{N} e^{f_k - u_k(\\vec{q})}}\\right\\rangle_{m}$.  \n",
    "\n",
    "and we can multiply both sides by $e^{-f_1}$, to get the implicit equation for $f_1$\n",
    "\n",
    "$ \\displaystyle e^{-f_1} = \\left\\langle \\frac{e^{-u_1(\\vec{q})}}{\\sum_{i=1}^K \\frac{N_k}{N} e^{f_k - u_k(\\vec{q})}}\\right\\rangle_{m}$.  \n",
    "\n",
    "$ \\displaystyle e^{-f_k} \\approx \\sum_{n=1}^N  \\frac{e^{-u_k(\\vec{q}_n)}}{\\sum_{i=1}^K \\frac{N_k}{N} e^{f_k - u_k(\\vec{q}_n)}}$.  \n",
    "\n",
    "We can write one of these equations for each $f_k$.  This set of implicit equations can clearly be seen as the MBAR equations. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8bd0bf-4260-484f-9284-0071e2f55aca",
   "metadata": {},
   "source": [
    "## Estimating Free Energies with MBAR\n",
    "MBAR provides the direct equation to find free energies as show above assuming you have the energies. This can be rather difficult to implement for beginners since this does require iterative solutions like BAR (note: see the [free, Python implementation](#dl_mbar) below). Despite this, MBAR still has the lowest variance of all the other methods listed under the [theory](fundamentals) section of the fundamentals. Further, MBAR has a direct way to calculate errors (see paper{cite}`Shirts2008` for derivation).\n",
    "\n",
    "You will need a complete set of $\\Delta U_{k,j}$ for all $K$ states for MBAR to work, just like you do in [](WHAM). Fortunately, you can do most of this in post processing if need be. Once again, you do not *need* to calculate $\\Delta U_{k,k}$, but it is still a good self check to ensure the reweighting method is working correctly; all the remaining checks for [WHAM](wham_usage) hold as well and should be followed when analyzing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db87126-3d5b-4e63-8bc7-3685cac3187e",
   "metadata": {},
   "source": [
    "(dl_mbar)=\n",
    "## Download MBAR\n",
    "MBAR may seem like a daunting set of equations to program yourself, so the authors have provided a Python implementation of MBAR for anyone to use, free of charge at http://github.com/choderalab/pymbar, with a number of examples for a variety of situations at  http://github.com/choderalab/pymbar-examples. The software comes with examples and uses cases. Also bundled with it are the tools to compute expectation values and an implementation of BAR."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
